import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

num_words = 1000
maxlen = 200

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

x_train = pad_sequences(x_train, maxlen=maxlen)
x_test  = pad_sequences(x_test, maxlen=maxlen)

x_train, x_val, y_train, y_val = train_test_split(
    x_train, y_train, test_size=0.2, random_state=42
)


model = models.Sequential([
    layers.Embedding(num_words, 128, input_length=maxlen),
    layers.Bidirectional(layers.LSTM(64)),
    layers.Dense(1, activation="sigmoid")
])

model.compile(optimizer="adam",
              loss="binary_crossentropy",
              metrics=["accuracy"])

history = model.fit(
    x_train, y_train,
    epochs=5, batch_size=128,
    validation_data=(x_val, y_val)
)


loss, acc = model.evaluate(x_val, y_val)
print(f"\nValidation Accuracy: {acc*100:.2f}%")

word_index = imdb.get_word_index()
reverse_word_index = {v: k for k, v in word_index.items()}

def decode_review(encoded):
    return " ".join([reverse_word_index.get(i - 3, "?") for i in encoded if i > 2])

def predict_sentiment(text):
    encoded = [word_index.get(w, 2) + 3 for w in text.split()]
    padded = pad_sequences([encoded], maxlen=maxlen)
    pred = model.predict(padded, verbose=0)[0][0]
    return "Positive" if pred >= 0.5 else "Negative"


print("\nSentiment Analysis of IMDB Dataset : \n")
print("\nSample Predictions:\n")
for i in range(5):
    review = decode_review(x_test[i])
    sentiment = predict_sentiment(review)
    print("="*60)
    print("Review:\n", review[:300], "...")
    print("Sentiment:", sentiment)
    print("="*60)
